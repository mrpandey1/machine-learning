{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.0.0\n",
      "TF Hub version:  0.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "print('TF version: ',tf.__version__)\n",
    "print('TF Hub version: ',hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_label(prediction_probability):\n",
    "  '''\n",
    "  Turns an array of predictions into label\n",
    "  '''\n",
    "  return unique_labels[np.argmax(prediction_probability)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels=['affenpinscher', 'afghan_hound', 'african_hunting_dog', 'airedale',\n",
    "       'american_staffordshire_terrier', 'appenzeller',\n",
    "       'australian_terrier', 'basenji', 'basset', 'beagle',\n",
    "       'bedlington_terrier', 'bernese_mountain_dog',\n",
    "       'black-and-tan_coonhound', 'blenheim_spaniel', 'bloodhound',\n",
    "       'bluetick', 'border_collie', 'border_terrier', 'borzoi',\n",
    "       'boston_bull', 'bouvier_des_flandres', 'boxer',\n",
    "       'brabancon_griffon', 'briard', 'brittany_spaniel', 'bull_mastiff',\n",
    "       'cairn', 'cardigan', 'chesapeake_bay_retriever', 'chihuahua',\n",
    "       'chow', 'clumber', 'cocker_spaniel', 'collie',\n",
    "       'curly-coated_retriever', 'dandie_dinmont', 'dhole', 'dingo',\n",
    "       'doberman', 'english_foxhound', 'english_setter',\n",
    "       'english_springer', 'entlebucher', 'eskimo_dog',\n",
    "       'flat-coated_retriever', 'french_bulldog', 'german_shepherd',\n",
    "       'german_short-haired_pointer', 'giant_schnauzer',\n",
    "       'golden_retriever', 'gordon_setter', 'great_dane',\n",
    "       'great_pyrenees', 'greater_swiss_mountain_dog', 'groenendael',\n",
    "       'ibizan_hound', 'irish_setter', 'irish_terrier',\n",
    "       'irish_water_spaniel', 'irish_wolfhound', 'italian_greyhound',\n",
    "       'japanese_spaniel', 'keeshond', 'kelpie', 'kerry_blue_terrier',\n",
    "       'komondor', 'kuvasz', 'labrador_retriever', 'lakeland_terrier',\n",
    "       'leonberg', 'lhasa', 'malamute', 'malinois', 'maltese_dog',\n",
    "       'mexican_hairless', 'miniature_pinscher', 'miniature_poodle',\n",
    "       'miniature_schnauzer', 'newfoundland', 'norfolk_terrier',\n",
    "       'norwegian_elkhound', 'norwich_terrier', 'old_english_sheepdog',\n",
    "       'otterhound', 'papillon', 'pekinese', 'pembroke', 'pomeranian',\n",
    "       'pug', 'redbone', 'rhodesian_ridgeback', 'rottweiler',\n",
    "       'saint_bernard', 'saluki', 'samoyed', 'schipperke',\n",
    "       'scotch_terrier', 'scottish_deerhound', 'sealyham_terrier',\n",
    "       'shetland_sheepdog', 'shih-tzu', 'siberian_husky', 'silky_terrier',\n",
    "       'soft-coated_wheaten_terrier', 'staffordshire_bullterrier',\n",
    "       'standard_poodle', 'standard_schnauzer', 'sussex_spaniel',\n",
    "       'tibetan_mastiff', 'tibetan_terrier', 'toy_poodle', 'toy_terrier',\n",
    "       'vizsla', 'walker_hound', 'weimaraner', 'welsh_springer_spaniel',\n",
    "       'west_highland_white_terrier', 'whippet',\n",
    "       'wire-haired_fox_terrier', 'yorkshire_terrier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "\n",
    "def create_data_batches(x,y=None,batch_size=BATCH_SIZE,valid_data=False,test_data=False):\n",
    "  '''\n",
    "  Create batched of data out of image and label pairs.\n",
    "  Shuffles the data if its training data but doesnt shuffle if its validation data\n",
    "  Also accepts test data as input\n",
    "  '''\n",
    "  # tf.constants -create a constant tensor from a tensor like object\n",
    "  if test_data:\n",
    "    print('Creating test data branches....')\n",
    "    data=tf.data.Dataset.from_tensor_slices((tf.constant(x)))# this says, pass me some\n",
    "    # tensor and I'll create dataset out of that\n",
    "    data_batch=data.map(preprocess_image).batch(BATCH_SIZE)\n",
    "    return data_batch\n",
    "  # If the data is valid dataset , we don't  need to shuffle it\n",
    "  elif valid_data:\n",
    "    print('Creating validation data batches....')\n",
    "    data=tf.data.Dataset.from_tensor_slices((tf.constant(x),# filepaths\n",
    "                                             tf.constant(y)))\n",
    "    data_batch = data.map(get_image_label).batch(BATCH_SIZE) # process the image and turn it into the batch\n",
    "    return data_batch\n",
    "  else:\n",
    "    print('create training data batches ...')\n",
    "    data=tf.data.Dataset.from_tensor_slices((tf.constant(x),# filepaths\n",
    "                                             tf.constant(y))) # labels\n",
    "    #process the image and turn it into batch\n",
    "    data_batch=data.map(get_image_label).batch(BATCH_SIZE)\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_in_one():\n",
    "    # Importing the model\n",
    "    model_path='200415-09441586943843-full-image-mobilenetV2.h5'\n",
    "    print(f'Loading model from :{model_path}')\n",
    "    loaded_model=tf.keras.models.load_model(model_path,\n",
    "                                   custom_objects={\n",
    "                                       'KerasLayer':hub.KerasLayer\n",
    "                                   })\n",
    "    # getting the images\n",
    "    custom_path='testing/'\n",
    "    custom_images_path=[custom_path+image for image in os.listdir(custom_path)]\n",
    "    # converting images into batches\n",
    "    custom_data=create_data_batches(custom_images_path,test_data=True)\n",
    "    # Prediting the value\n",
    "    custom_preds=loaded_model.predict(custom_data)\n",
    "    custom_pred_label=[get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]\n",
    "    return custom_pred_label,custom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to unbatch dataset\n",
    "def unbatchify(batch_dataset):\n",
    "    images=[]\n",
    "    for image in batch_dataset.unbatch().as_numpy_iterator():\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from :200415-09441586943843-full-image-mobilenetV2.h5\n",
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data branches....\n",
      "<BatchDataset shapes: (None, 224, 224, 3), types: tf.float32>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'as_numpy_iterator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-0279394919ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mshow_4_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mshow_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-99-0279394919ad>\u001b[0m in \u001b[0;36mshow_result\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_in_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mshow_4_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mshow_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'as_numpy_iterator'"
     ]
    }
   ],
   "source": [
    "from IPython.display import  Image\n",
    "def show_result():\n",
    "    results,train_data=all_in_one()\n",
    "    print(train_data)\n",
    "    images=next(train_data.as_numpy_iterator())\n",
    "    show_4_images(images)\n",
    "show_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_4_images(images):\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for i in range(5):\n",
    "    ax=plt.subplot(5,5,i+1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
